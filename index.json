
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Federico Errica received his PhD in Computer Science from the University of Pisa, supervised by Alessio Micheli and Davide Bacciu. He is now a Research Scientist at NEC Laboratories Europe GmbH. His research interests include deep probabilistic models for graphs, neural networks and hybrid architectures.\nDownload: PhD thesis ","date":1705276800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1705276800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Federico Errica received his PhD in Computer Science from the University of Pisa, supervised by Alessio Micheli and Davide Bacciu. He is now a Research Scientist at NEC Laboratories Europe GmbH.","tags":null,"title":"Federico Errica","type":"authors"},{"authors":["Federico Errica"],"categories":["Research"],"content":"Graph-Induced Sum-Product Networks have been (finally!) accepted at ICLR 2024! Blog post coming in the following months… stay tuned! Special thanks to Mathias Niepert for his relentless support =).\n","date":1705276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705276800,"objectID":"7cb116e48bb62f091f2f035676351277","permalink":"https://diningphil.github.io/post/24_iclr/","publishdate":"2024-01-15T00:00:00Z","relpermalink":"/post/24_iclr/","section":"post","summary":"Graph-Induced Sum-Product Networks have been (finally!) accepted at ICLR 2024! Blog post coming in the following months… stay tuned! Special thanks to Mathias Niepert for his relentless support =).","tags":["Deep Graph Networks","Probabilistic Models"],"title":"Paper accepted at ICLR 2024!!","type":"post"},{"authors":["Federico Errica","Mathias Niepert"],"categories":null,"content":"We propose a probabilistic model that bridges graph machine learning and sum-product networks to tractably answer probabilistic queries. Our Graph-induced Sum-Product Network can solve unsupervised and supervised graph tasks and it is especially effective in modeling the missing data distribution as well as exploiting unlabeled data in a scarce supervision scenario.\n","date":1705276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705276800,"objectID":"a646afd8bdaac855c835f5a4a06f6112","permalink":"https://diningphil.github.io/publication/2024_iclr/","publishdate":"2024-01-15T00:00:00Z","relpermalink":"/publication/2024_iclr/","section":"publication","summary":"We propose a probabilistic model that bridges graph machine learning and sum-product networks to tractably answer probabilistic queries. Our Graph-induced Sum-Product Network can solve unsupervised and supervised graph tasks and it is especially effective in modeling the missing data distribution as well as exploiting unlabeled data in a scarce supervision scenario.","tags":["Deep Graph Networks","Probabilistic Models"],"title":"Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks","type":"publication"},{"authors":["Federico Errica"],"categories":null,"content":"In this paper I try to address the following question: “Is it really worth to build a k-Nearest Neighbor graph from tabular data and then apply deep graph nets on top of it to classify each sample in the table?” This is something people tend to do, and I have always been skeptical about it. Now we have a very good indication, which I also tried to justify theoretically, that a k-NN graph is not the best structure to use if we want to gain some real advantages (under some mild conditions). I also argue that new methods of building synthetic graphs are necessary if we want to exploit some “latent” structure between the samples of a dataset.\n","date":1702684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702684800,"objectID":"504d235fb7e3424d7dacb56aefa4202a","permalink":"https://diningphil.github.io/publication/2023_neurips/","publishdate":"2023-09-24T00:00:00Z","relpermalink":"/publication/2023_neurips/","section":"publication","summary":"In this paper I try to address the following question: “Is it really worth to build a k-Nearest Neighbor graph from tabular data and then apply deep graph nets on top of it to classify each sample in the table?","tags":[],"title":"On Class Distributions Induced by Nearest Neighbor Graphs for Node Classification of Tabular Data","type":"publication"},{"authors":["Federico Errica"],"categories":["Research"],"content":"Our work on learning to accelerate Hamiltonian Monte Carlo simulations has been accepted at the Journal of Chemical Physics! Shout out to my colleagues Henrik and Francesco for the hard work =)\nStay tuned for more work on machine learning for the computational sciences =).\n","date":1701043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701043200,"objectID":"b1e1d6253533c536894eeb9a9e77c2be","permalink":"https://diningphil.github.io/post/23_jcp/","publishdate":"2023-11-27T00:00:00Z","relpermalink":"/post/23_jcp/","section":"post","summary":"Our work on learning to accelerate Hamiltonian Monte Carlo simulations has been accepted at the Journal of Chemical Physics! Shout out to my colleagues Henrik and Francesco for the hard work =)","tags":["Hamiltonian Monte Carlo","Simulations"],"title":"Paper accepted at JCP!!","type":"post"},{"authors":["Henrik Christiansen","Federico Errica","Francesco Alesiani"],"categories":null,"content":"In this paper we accelerate HMC simulations by learning atom-dependent timesteps and the number of molecular dynamics steps! With an appropriate choice of the loss, we can significantly speed up simulations and reduce autocorrelation times by 25% on alanine dipeptide.\n","date":1701043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701043200,"objectID":"65cee45c5382e9622fdc75deec29dce0","permalink":"https://diningphil.github.io/publication/2023_jcp/","publishdate":"2023-11-27T00:00:00Z","relpermalink":"/publication/2023_jcp/","section":"publication","summary":"In this paper we accelerate HMC simulations by learning atom-dependent timesteps and the number of molecular dynamics steps! With an appropriate choice of the loss, we can significantly speed up simulations and reduce autocorrelation times by 25% on alanine dipeptide.","tags":[],"title":"Self-Tuning Hamiltonian Monte Carlo for Accelerated Sampling","type":"publication"},{"authors":["Federico Errica","Alessio Gravina","Davide Bacciu","Alessio Micheli"],"categories":null,"content":"We extend HMMs to graphs!\n","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"601006b7a1a8139b4e5f82a2439bed56","permalink":"https://diningphil.github.io/publication/2023_esann/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/publication/2023_esann/","section":"publication","summary":"Who said HMMs cannot be adapted for graph-structured data? In this work, we extend CGMM to the temporal domain, and we report performances close to other popular neural models with a probabilistic graph embedding model!","tags":[],"title":"Hidden Markov Models for Temporal Graph Representation Learning","type":"publication"},{"authors":["Federico Errica"],"categories":["Software"],"content":"PyDGN just got accepted in Journal of Open Source Software =)\nI have to say, compared to other prestigious journals, I got much more constructive feedback and the review process was faster and transparent. Thank you to the editor and the reviewers!\n","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"1fee6e18219056063660b1ddb723b17f","permalink":"https://diningphil.github.io/post/23_joss/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/post/23_joss/","section":"post","summary":"PyDGN just got accepted in Journal of Open Source Software =)\nI have to say, compared to other prestigious journals, I got much more constructive feedback and the review process was faster and transparent.","tags":["Deep Graph Networks","Reproducibility"],"title":"PyDGN accepted at JOSS!","type":"post"},{"authors":["Federico Errica","Davide Bacciu","Alessio Micheli"],"categories":null,"content":"This is a Python library to easily experiment with Deep Graph Networks (DGNs). It provides automatic management of data splitting, loading and common experimental settings. It also handles both model selection and risk assessment procedures, by trying many different configurations in parallel (CPU or GPU).\n","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"6c8e88e6537ba2a77d162ce2e52c62ba","permalink":"https://diningphil.github.io/publication/2023_joss/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/publication/2023_joss/","section":"publication","summary":"A Python library to easily experiment with Deep Graph Networks (DGNs)","tags":[],"title":"PyDGN: a Python Library for Flexible and Reproducible Research on Deep Learning for Graphs","type":"publication"},{"authors":["Federico Errica"],"categories":["Research"],"content":"I am really excited: my very first single-author paper has been accepted at NeurIPS 2023! I still can’t believe that I managed to pull this off, especially after submitting it for the first time =) give it a few weeks for the papers to be released on OpenReview!\n","date":1695513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695513600,"objectID":"2c25a2e1eb3e431ced088011e2001918","permalink":"https://diningphil.github.io/post/23_neurips/","publishdate":"2023-09-24T00:00:00Z","relpermalink":"/post/23_neurips/","section":"post","summary":"I am really excited: my very first single-author paper has been accepted at NeurIPS 2023! I still can’t believe that I managed to pull this off, especially after submitting it for the first time =) give it a few weeks for the papers to be released on OpenReview!","tags":["Deep Graph Networks"],"title":"Paper accepted at NeurIPS 2023!!","type":"post"},{"authors":["Daniele Castellana","Federico Errica"],"categories":null,"content":"We investigate the quality of quantitative measures that assess the utility of a graph structure for node classification tasks.\n","date":1694131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694131200,"objectID":"51c8fc9df258b9f12a289adba9a21465","permalink":"https://diningphil.github.io/publication/2023_ecml/","publishdate":"2023-09-08T00:00:00Z","relpermalink":"/publication/2023_ecml/","section":"publication","summary":"We investigate the quality of quantitative measures that assess the utility of a graph structure for node classification tasks.","tags":[],"title":"Investigating the Interplay between Features and Structures in Graph Learning","type":"publication"},{"authors":[],"categories":null,"content":"\r","date":1677157200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677157200,"objectID":"a35ec7107a2ba5bbf672079aab3827af","permalink":"https://diningphil.github.io/talk/phd-course-on-probabilistic-reasoning-in-ml/","publishdate":"2023-02-23T13:00:00Z","relpermalink":"/talk/phd-course-on-probabilistic-reasoning-in-ml/","section":"event","summary":"Together with Daniele Castellana, we taught sampling, and variational inference approaches for Latent Variable Models (LVMs), as well as an introduction to infinite LVMs and LVMs for structured data such as trees and graphs.","tags":[],"title":"PhD Course on Probabilistic Reasoning in ML","type":"event"},{"authors":["Federico Errica"],"categories":["Research"],"content":"After a year of hard work, rejections, good feedback, paper rewriting, and additional experiments, our Infinite Contextual Graph Markov Model has been accepted at ICML 2022!\niCGMM combines graph learning and Bayesian nonparametric to build a deep model that can decide the complexity of each of its layers and automatize the choice of its hyper-parameters during training.\nNeedless to say, this work wouldn’t have been possible without the incredible expertise of Daniele Castellana about BNP methods and the supervision of Davide Bacciu and Alessio Micheli. Kudos to this incredible team!\nIf you don’t want to wait for the proceedings, please consider taking a look at Section 4.3 of my PhD thesis (link at the top of the homepage).\n","date":1652572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652572800,"objectID":"6045b13fe49bad2e9e5f216e36f38089","permalink":"https://diningphil.github.io/post/22_icgmm/","publishdate":"2022-05-15T00:00:00Z","relpermalink":"/post/22_icgmm/","section":"post","summary":"Infinite Contextual Graph Markov Model","tags":["Deep Graph Networks","Bayesian Nonparametrics","Probabilistic Models"],"title":"Paper accepted at ICML 2022!!","type":"post"},{"authors":["Daniele Castellana","Federico Errica","Davide Bacciu","Alessio Micheli"],"categories":null,"content":"Combining Deep Graph Networks with Bayesian nonparametric techniques to unsupervised learning of node/graph representations while automatizing the choice of the hyper-parameters during training.\n","date":1652572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652572800,"objectID":"eeb78782bd22af64cd88df730909f561","permalink":"https://diningphil.github.io/publication/2022_icml/","publishdate":"2022-05-15T00:00:00Z","relpermalink":"/publication/2022_icml/","section":"publication","summary":"Combining Deep Graph Networks with Bayesian nonparametric techniques to unsupervised learning of node/graph representations while automatizing the choice of the hyper-parameters during training.","tags":[],"title":"The Infinite Contextual Graph Markov Model","type":"publication"},{"authors":["Luca Oneto","Nicolo Navarin","Battista Biggio","Federico Errica","Alessio Micheli","Franco Scarselli","Monica Bianchini","Luca Demetrio","Pietro Bongini","Armando Tacchella","Alessandro Sperduti"],"categories":null,"content":"An overview of the current works focused towards learning trustworthily, automatically, and with guarantees on graphs.\n","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649980800,"objectID":"902c3af455ed1e8aaaa778e5dce77732","permalink":"https://diningphil.github.io/publication/2022_neurocomputing/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/publication/2022_neurocomputing/","section":"publication","summary":"An overview of the current works focused towards learning trustworthily, automatically, and with guarantees on graphs.","tags":[],"title":"Towards Learning Trustworthily, Automatically, and with Guarantees on Graphs: an Overview","type":"publication"},{"authors":["Federico Errica"],"categories":["Software"],"content":"PyDGN 1.0.0 is out! We have added a documentation, refactored the code, implemented a bunch of user-friendly features and provided clean, well commented configuration files!\n","date":1646179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646179200,"objectID":"2d34e59d5134ef1246af4474064c1d02","permalink":"https://diningphil.github.io/post/22_pydgn-1.0.0/","publishdate":"2022-03-02T00:00:00Z","relpermalink":"/post/22_pydgn-1.0.0/","section":"post","summary":"PyDGN 1.0.0 is out! We have added a documentation, refactored the code, implemented a bunch of user-friendly features and provided clean, well commented configuration files!","tags":["Deep Graph Networks","Reproducibility"],"title":"PyDGN 1.0.0","type":"post"},{"authors":["Antonio Carta","Andrea Cossu","Federico Errica","Davide Bacciu"],"categories":null,"content":"We study the phenomenon of catastrophic forgetting in the graph representation learning scenario.\n","date":1643760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643760000,"objectID":"d05615f6730710a87dc76efc5613fc2b","permalink":"https://diningphil.github.io/publication/2022_frontiers/","publishdate":"2022-02-02T00:00:00Z","relpermalink":"/publication/2022_frontiers/","section":"publication","summary":"We study the phenomenon of catastrophic forgetting in the graph representation learning scenario.","tags":[],"title":"Catastrophic Forgetting in Deep Graph Networks: A Graph Classification Benchmark","type":"publication"},{"authors":["Federico Errica"],"categories":[],"content":"While I prepare my Ph.D. thesis’ defense, I have just started working for NEC Laboratories Europe as a Research Scientist! The people and the environment are truly stimulating, stay tuned for more updates on deep learning for graphs =).\n","date":1642032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642032000,"objectID":"173007e09dcbbadba4a18cc6e1d5cfd7","permalink":"https://diningphil.github.io/post/22_nec/","publishdate":"2022-01-13T00:00:00Z","relpermalink":"/post/22_nec/","section":"post","summary":"While I prepare my Ph.D. thesis’ defense, I have just started working for NEC Laboratories Europe as a Research Scientist! The people and the environment are truly stimulating, stay tuned for more updates on deep learning for graphs =).","tags":[],"title":"Started a new job","type":"post"},{"authors":["Federico Errica","Giacomo Iadarola","Fabio Martinelli","Francesco Mercaldo","Alessio Micheli"],"categories":null,"content":"","date":1631318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631318400,"objectID":"be1fd2de3f2ed589c2d4db66afdeb49a","permalink":"https://diningphil.github.io/publication/2021_esann/","publishdate":"2021-09-11T00:00:00Z","relpermalink":"/publication/2021_esann/","section":"publication","summary":"","tags":[],"title":"Robust Malware Classification via Deep Graph Networks on Call Graph Topologies","type":"publication"},{"authors":["Federico Errica"],"categories":["Software"],"content":"We officially released the 0.5.0 version of our PyDGN library! We have added a bunch of useful features, refactored the code, and fixed some bugs! Special thanks to Danilo Numeroso who implemented a very useful and flexible random search technique!\n","date":1628812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628812800,"objectID":"45b911e9da2263f1f61e3763847b78ac","permalink":"https://diningphil.github.io/post/21_pydgn050/","publishdate":"2021-08-13T00:00:00Z","relpermalink":"/post/21_pydgn050/","section":"post","summary":"PyDGN v0.5.0 is out","tags":["Deep Graph Networks","Reproducibility"],"title":"PyDGN v0.5.0 is out!","type":"post"},{"authors":[],"categories":null,"content":" ","date":1626872400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626872400,"objectID":"6128da41f56555a02252cad8e8733a96","permalink":"https://diningphil.github.io/talk/nec-labs-europe/","publishdate":"2021-07-21T00:00:00Z","relpermalink":"/talk/nec-labs-europe/","section":"event","summary":"I talked about my PhD research","tags":[],"title":"NEC Labs Europe","type":"event"},{"authors":["Federico Errica","Davide Bacciu","Alessio Micheli"],"categories":null,"content":"Combining Deep Graph Networks with Mixture Density Networks to model multimodal output distributions conditioned on arbitrary input graphs.\n","date":1620518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620518400,"objectID":"6fe4058c19abc3d2e5e08f61f4c7c00c","permalink":"https://diningphil.github.io/publication/2021_icml/","publishdate":"2021-05-09T00:00:00Z","relpermalink":"/publication/2021_icml/","section":"publication","summary":"Combining Deep Graph Networks with Mixture Density Networks to model multimodal output distributions conditioned on arbitrary input graphs.","tags":[],"title":"Graph Mixture Density Networks","type":"publication"},{"authors":["Federico Errica"],"categories":["Research"],"content":"Wonderful news! Our paper “Graph Mixture Density Networks” has been accepted at ICML 2021! Shout out to my supervisors Alessio Micheli and Davide Bacciu that made this possible. We study the problem of learning multi-modal output distributions conditioned on arbitrary input graphs. With GMDN, we can predict if there’s more than one likely outcome associated with an input graph: this is especially useful, for example, when predicting the final outcome of a pandemic when the social network is known. We will release the camera ready very soon!\n","date":1620518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620518400,"objectID":"d33a92f32a285e1aa2644c3b831ce78f","permalink":"https://diningphil.github.io/post/21_icml/","publishdate":"2021-05-09T00:00:00Z","relpermalink":"/post/21_icml/","section":"post","summary":"Graph Mixture Density Network","tags":["Deep Graph Networks","Probabilistic Models"],"title":"Paper accepted at ICML 2021!!","type":"post"},{"authors":["Federico Errica"],"categories":["Research"],"content":"Two papers accepted at IJCNN 2021! The first, “Modeling Edge Features with Deep Graph Bayesian Networks”, extends the Contextual Graph Markov Model to the processing of arbitrary edge features! The second, “Concept Matching for Low-resource Classification”, presents a new way to train prototypes of important words to perform classification when supervised data is limited. You can find the papers on my publication list! Congrats to all my co-authors!\n","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"7649e64d75ffb4c38cbcaf69556b03f8","permalink":"https://diningphil.github.io/post/21_ijcnn/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/post/21_ijcnn/","section":"post","summary":"Deep Bayesian Graph Networks \u0026 Low-resource Classification","tags":["Deep Graph Networks","Probabilistic Models"],"title":"2 Papers accepted at IJCNN 2021!","type":"post"},{"authors":["Federico Errica","Fabrizio Silvestri","Bora Edizel","Ludovic Denoyer","Fabio Petroni","Vassilis Plachouras","Sebastian Riedel"],"categories":null,"content":"","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"438ab5df44132dabf42eb40c0c7f428c","permalink":"https://diningphil.github.io/publication/2021_ijcnn_parcus/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/publication/2021_ijcnn_parcus/","section":"publication","summary":"","tags":[],"title":"Concept Matching for Low-Resource Classification","type":"publication"},{"authors":["Daniele Atzeni","Davide Bacciu","Federico Errica","Alessio Micheli"],"categories":null,"content":"","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"e4b7f7da4a592869d612ff945af02e8d","permalink":"https://diningphil.github.io/publication/2021_ijcnn_ecgmm/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/publication/2021_ijcnn_ecgmm/","section":"publication","summary":"","tags":[],"title":"Modeling Edge Features with Deep Bayesian Graph Networks","type":"publication"},{"authors":[],"categories":null,"content":" ","date":1617973200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617973200,"objectID":"ae0b3e02c93fc8a67c0da32244586b0b","permalink":"https://diningphil.github.io/talk/continual-ai/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/continual-ai/","section":"event","summary":"I talked about our latest work on catastrophic forgetting for DGNs.","tags":[],"title":"Continual AI","type":"event"},{"authors":[],"categories":null,"content":" ","date":1617714000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617714000,"objectID":"ee8d9c7288d8db890c30859eca8469f1","permalink":"https://diningphil.github.io/talk/ibm-research-zurich/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/ibm-research-zurich/","section":"event","summary":"I talked about our latest research on deep learning for graphs.","tags":[],"title":"IBM Research Zurich","type":"event"},{"authors":["Federico Errica"],"categories":["Research"],"content":"Together with Antonio Carta, Andrea Cossu, and Davide Bacciu, our paper “Catastrophic Forgetting in Deep Graph Networks: an Introductory Benchmark for Graph Classification” has been accepted for publication at the WWW\u0026#39;21 Workshop on Graph Learning Benchmarks. We study whether known continual learning techniques have an impact when applied to graphs, but we also observe that structure-preserving regularization may help. Also, a structure-agnostic baseline shows strong performances in this scenario (again), indicating that there is much to be done at the intersection of the Continual Learning and Graph Representation Learning fields! EDIT: it has also been selected as one of the two orals of the workshop! Great news :)\n","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615939200,"objectID":"7bb0fa8fcaeb33c70b3dbe1b3e5b51a5","permalink":"https://diningphil.github.io/post/21_workshopwww/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/post/21_workshopwww/","section":"post","summary":"Continual Learning for Graphs","tags":["Deep Graph Networks","Continual Learning"],"title":"Paper accepted at WWW 2021 Workshop!","type":"post"},{"authors":["Federico Errica"],"categories":["Research"],"content":"Our paper “A deep graph network-enhanced sampling approach to efficiently explore the space of reduced representations of proteins” has been accepted for publication in Frontiers Molecular Biosciences. Together with the Potestio Lab, we trained a deep graph networks to approximate a complex measure of information retention in proteins after a coarse-graining process is applied. PDF available soon!\n","date":1615248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615248000,"objectID":"00ba5af8be314ded5351482155a7a7cb","permalink":"https://diningphil.github.io/post/21_pisatrento/","publishdate":"2021-03-09T00:00:00Z","relpermalink":"/post/21_pisatrento/","section":"post","summary":"Approximating information loss on chemical molecules","tags":["Deep Graph Networks","Coarse Graining","Simulations"],"title":"Paper accepted at Frontiers Molecular Biosciences!","type":"post"},{"authors":["Federico Errica","Marco Giulini","Davide Bacciu","Roberto Menichetti","Alessio Micheli","Raffaello Potestio"],"categories":null,"content":"Learning complex measure of information loss in reduce representations of proteins via deep graph networks.\n","date":1613520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613520000,"objectID":"6f2c104bcaf4d62cf19306d64cac1678","permalink":"https://diningphil.github.io/publication/2021_frontiers/","publishdate":"2021-02-17T00:00:00Z","relpermalink":"/publication/2021_frontiers/","section":"publication","summary":"Learning complex measure of information loss in reduce representations of proteins via deep graph networks.","tags":[],"title":"A deep graph network-enhanced sampling approach to efficiently explore the space of reduced representations of proteins","type":"publication"},{"authors":["Antonio Carta","Andrea Cossu","Federico Errica","Davide Bacciu"],"categories":null,"content":"An empirical work at the intersection of Continual Learning and Graph Representation Learning. Selected as one of the two oral presentations.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"ebfe60b539137cb5cdb9b4e4ebd8cfca","permalink":"https://diningphil.github.io/publication/2021_grl_www/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/publication/2021_grl_www/","section":"publication","summary":"An empirical work at the intersection of Continual Learning and Graph Representation Learning. Selected as one of the two oral presentations.","tags":[],"title":"Catastrophic Forgetting in Deep Graph Networks: an Introductory Benchmark for Graph Classification","type":"publication"},{"authors":["Federico Errica","Davide Bacciu","Alessio Micheli"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"03a96dadb856fdc2ba4f1f0dd086ecb4","permalink":"https://diningphil.github.io/publication/2020_esann/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/2020_esann/","section":"publication","summary":"","tags":[],"title":"Theoretically Expressive and Edge-aware Graph Learning","type":"publication"},{"authors":["Federico Errica"],"categories":["Software"],"content":"We officially released the 0.4.0 version of our PyDGN library! In the past few months the library has become more mature, and I summarized its main features in this short presentation . Version 0.4.0 features the addition of the Ray library to handle multiprocessing as well as distributed computing! To make an example, you can run parallel experiments on your local laptop, on different GPUs at the same time or even on a cluster of machines! Special thanks to Antonio Carta who made a wonderful porting of Ray into PyDGN! Now…back to research! :)\n","date":1599955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599955200,"objectID":"1ce86ba3c8280cd93b4173865dd56cc9","permalink":"https://diningphil.github.io/post/20_pydgn040/","publishdate":"2020-09-13T00:00:00Z","relpermalink":"/post/20_pydgn040/","section":"post","summary":"PyDGN v0.4.0 is out","tags":["Deep Graph Networks","Reproducibility"],"title":"PyDGN v0.4.0 is out!","type":"post"},{"authors":["Federico Errica","Marco Giulini","Davide Bacciu","Roberto Menichetti","Alessio Micheli","Raffaello Potestio"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"b28647958c8944d33f5faa04a9879811","permalink":"https://diningphil.github.io/publication/2020_pisa_trento/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/2020_pisa_trento/","section":"publication","summary":"","tags":[],"title":"Accelerating the identification of informative reduced representations of proteins with deep learning for graphs","type":"publication"},{"authors":["Davide Bacciu","Federico Errica","Alessio Micheli"],"categories":null,"content":"Further studies on the Contextual Graph Markov Model.\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"7f4a3f3327d344a3d960aa21118d4173","permalink":"https://diningphil.github.io/publication/2020_jmlr/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_jmlr/","section":"publication","summary":"Further studies on the Contextual Graph Markov Model.","tags":[],"title":"Probabilistic Learning on Graphs via Contextual Architectures","type":"publication"},{"authors":["Davide Bacciu","Federico Errica","Marco Podda","Alessio Micheli"],"categories":null,"content":"A top-down approach to the field of machine learning for graphs.\n","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"c51f82fb41d714de3ac48600fc9e0931","permalink":"https://diningphil.github.io/publication/2020_neunet/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/2020_neunet/","section":"publication","summary":"A top-down approach to the field of machine learning for graphs.","tags":[],"title":"A Gentle Introduction to Deep Learning for Graphs","type":"publication"},{"authors":["Federico Errica","Marco Podda","Davide Bacciu","Alessio Micheli"],"categories":null,"content":"A fair and robust evaluation of Deep Graph Networks for graph classification tasks.\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"b2947a7ddc7c05e60afd4f41f674fc7a","permalink":"https://diningphil.github.io/publication/2020_iclr/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/publication/2020_iclr/","section":"publication","summary":"A fair and robust evaluation of Deep Graph Networks for graph classification tasks.","tags":[],"title":"A Fair Comparison of Graph Neural Networks for Graph Classification","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Hugo Blox Builder Hugo Blox Builder | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://diningphil.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Hugo Blox Builder's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"One of the most popular articles of the last years in the Machine Learning community is the Auto-Encoding Variational Bayes, which also includes the so-called reparametrization trick. Intrigued by what was sketched in the article, I decided to work out the details of this reparametrization, covering 2 of the 3 cases described (but I guess the third one can be derived from the first 2). I want to share my calculations with you. I have learned (and reviewed) a lot in the process, which was not directly related to the equations themselves. Sharing is the only way to make real progress as a community. Have a look Here , and please contact me in case the steps are not correct or contain imprecise notation.\nUpdate, 2018-12-10: I have added the solution of the integrals of Appendix B. This was really useful to get familiar with the gaussian integral and Fubini’s theorem.\nSpecial thanks to Iacopo Ripoli for his decisive and helpful insights!\n","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"dbd93246fc1eb8e3330bf716b46269c4","permalink":"https://diningphil.github.io/project/reparamtrick/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/project/reparamtrick/","section":"project","summary":"One of the most popular articles of the last years in the Machine Learning community is the Auto-Encoding Variational Bayes, which also includes the so-called reparametrization trick. Intrigued by what was sketched in the article, I decided to work out the details of this reparametrization, covering 2 of the 3 cases described (but I guess the third one can be derived from the first 2).","tags":["Variational Auto-Encoders"],"title":"Notes on the Reparametrization Trick","type":"project"},{"authors":null,"categories":null,"content":"Recently I was looking at Stochastic Neighbor Embedding (SNE) and its t-distributed version (t-SNE), but I could not find the exact steps to derive the gradient of the loss function (there are small errors in the t-SNE article and no info in the SNE one), so I decided to carry on the derivation and share it. I hope this can be of any help to those who are studying the same topic. Here you find the pdf: please let me know if you spot an error! (Picture taken from t-SNE paper)\n","date":1543363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543363200,"objectID":"9766a6c3ce0353c95f6543376e1db216","permalink":"https://diningphil.github.io/project/tsnederivation/","publishdate":"2018-11-28T00:00:00Z","relpermalink":"/project/tsnederivation/","section":"project","summary":"Recently I was looking at Stochastic Neighbor Embedding (SNE) and its t-distributed version (t-SNE), but I could not find the exact steps to derive the gradient of the loss function (there are small errors in the t-SNE article and no info in the SNE one), so I decided to carry on the derivation and share it.","tags":["Dimensionality Reduction"],"title":"Derivations of SNE and t-SNE","type":"project"},{"authors":["Davide Bacciu","Federico Errica","Alessio Micheli"],"categories":null,"content":"A deep, probabilistic, and scalable machine learning model for graphs. Work done as part of my master thesis.\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"7deb69ff69cdf1373be7914e8f7e79f7","permalink":"https://diningphil.github.io/publication/2018_icml/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/publication/2018_icml/","section":"publication","summary":"A deep, probabilistic, and scalable machine learning model for graphs. Work done as part of my master thesis.","tags":[],"title":"Contextual Graph Markov Model: a Deep and Generative Approach to Graph Processing","type":"publication"}]